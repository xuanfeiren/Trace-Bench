#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''trainers_BENCHMARK.py
Run Trace trainers on benchmark LLM4AD tasks (generated by convert_llm4ad_benchmark.py).

This script works with benchmark task directories that contain self-contained
task modules with embedded evaluators.

Examples:
    pyt    print(f"\nResults saved to {csv_path}")
    if len(task_keys) == 1:
        print(f"TensorBoard logs saved to ./logs/{task_keys[0]}/")
    else:
        print(f"TensorBoard logs saved to ./logs/ (multiple task subdirectories)")
        for task_key in task_keys:
            print(f"  - ./logs/{task_key}/")n trainers_BENCHMARK.py --tasks ./benchmark_tasks --task circle_packing
    python trainers_BENCHMARK.py --tasks ./benchmark_tasks --task online_bin_packing_local --algos PrioritySearch --ps-steps 5
'''

from __future__ import annotations

import argparse, json, importlib.util, sys, time, csv, os, threading
from pathlib import Path
from typing import Dict, Any, List, Tuple
from datetime import datetime

import numpy as np

from opto import trace
from opto import trainer
from opto.trainer.algorithms.gepa_algorithms import GEPAAlgorithmBase, GEPAUCBSearch, GEPABeamPareto
from opto.features.priority_search import PrioritySearch as SearchAlgorithm
from opto.trainer.loggers import TensorboardLogger


class TimeoutError(Exception):
    """Custom timeout exception"""
    pass


def run_with_timeout(task_func, timeout_seconds=300):
    """Run a task function with timeout using threading."""
    result = [None]
    exception = [None]
    
    def target():
        try:
            result[0] = task_func()
        except Exception as e:
            exception[0] = e
    
    thread = threading.Thread(target=target)
    thread.daemon = True  # Dies when main thread dies
    thread.start()
    thread.join(timeout=timeout_seconds)
    
    if thread.is_alive():
        # Timeout occurred - we can't actually kill the thread, but we can return timeout error
        raise TimeoutError(f"Task timed out after {timeout_seconds} seconds")
    
    if exception[0] is not None:
        raise exception[0]
    
    return result[0]


# -------------------------------- Utilities --------------------------------

def load_benchmark_task(task_dir: Path):
    '''Load an benchmark task module from its directory.'''
    init_file = task_dir / '__init__.py'
    if not init_file.exists():
        raise FileNotFoundError(f"No __init__.py found in {task_dir}")
    
    spec = importlib.util.spec_from_file_location(task_dir.name, str(init_file))
    mod = importlib.util.module_from_spec(spec)
    sys.modules[spec.name] = mod
    spec.loader.exec_module(mod)
    return mod

def pick_benchmark_task(tasks_dir: Path, task_key: str) -> Path:
    '''
    Resolve an benchmark task directory by fuzzy key.
    '''
    cands = [p for p in tasks_dir.iterdir() if p.is_dir()]
    # exact
    for p in cands:
        if p.name == task_key:
            return p
    # substring
    for p in cands:
        if task_key in p.name:
            return p
    raise FileNotFoundError(f'No benchmark task matching: {task_key} in {tasks_dir}')

# -------------------------------- Bench core --------------------------------

def run_one(mod, algo_name: str, algo_cls, *, threads: int, optimizer_kwargs: Dict[str, Any], trainer_overrides: Dict[str, Any], task_name: str) -> Tuple[float, float, Dict[str, Any]]:
    '''Run a single algorithm on the benchmark task defined by `mod`.'''
    bundle = mod.build_trace_problem(**trainer_overrides.get('eval_kwargs', {}))
    param = bundle['param']
    guide = bundle['guide']
    ds = bundle['train_dataset']
    opt_kwargs = (bundle.get('optimizer_kwargs', {}) | (optimizer_kwargs or {}))
    
    # Store initial parameters for logging
    initial_params = getattr(param, 'data', None)
    
    # Setup TensorBoard logging
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir = f'./logs/{task_name}/{algo_name}/{timestamp}'
    logger = TensorboardLogger(log_dir=log_dir)

    # Algorithm params following priority_search_on_convex_fn_BENCH.py style
    if algo_name == 'PrioritySearch':
        params = dict(
            guide=guide,
            train_dataset=ds,
            score_range=[-10, 10],
            num_epochs=1,
            num_steps=trainer_overrides.get('ps_steps', 3),
            batch_size=1,
            num_batches=trainer_overrides.get('ps_batches', 2),
            verbose=False,
            num_candidates=trainer_overrides.get('ps_candidates', 4),
            num_proposals=trainer_overrides.get('ps_proposals', 4),
            memory_update_frequency=trainer_overrides.get('ps_mem_update', 2),
            optimizer_kwargs=opt_kwargs,
            num_threads=threads,
        )
    elif algo_name == 'GEPA-Base':
        params = dict(
            guide=guide,
            train_dataset=ds,
            validate_dataset=ds,
            num_iters=trainer_overrides.get('gepa_iters', 3),
            train_batch_size=trainer_overrides.get('gepa_train_bs', 2),
            merge_every=trainer_overrides.get('gepa_merge_every', 2),
            pareto_subset_size=trainer_overrides.get('gepa_pareto_subset', 4),
            num_threads=threads,
            optimizer_kwargs=opt_kwargs,
        )
    elif algo_name == 'GEPA-UCB':
        params = dict(
            guide=guide,
            train_dataset=ds,
            num_search_iterations=trainer_overrides.get('gepa_iters', 3),
            train_batch_size=trainer_overrides.get('gepa_train_bs', 2),
            merge_every=trainer_overrides.get('gepa_merge_every', 2),
            pareto_subset_size=trainer_overrides.get('gepa_pareto_subset', 4),
            num_threads=threads,
            optimizer_kwargs=opt_kwargs,
        )
    elif algo_name == 'GEPA-Beam':
        params = dict(
            guide=guide,
            train_dataset=ds,
            validate_dataset=ds,
            num_search_iterations=trainer_overrides.get('gepa_iters', 3),
            train_batch_size=trainer_overrides.get('gepa_train_bs', 2),
            merge_every=trainer_overrides.get('gepa_merge_every', 2),
            pareto_subset_size=trainer_overrides.get('gepa_pareto_subset', 4),
            num_threads=threads,
            optimizer_kwargs=opt_kwargs,
        )
    else:
        raise ValueError(f'Unknown algorithm name: {algo_name}')

    # Add logger to params
    params['logger'] = logger
    
    # The model is just the single ParameterNode (train wraps it into a Module)
    start = time.time()
    
    # Get timeout from task configuration or use default
    task_timeout = trainer_overrides.get('eval_kwargs', {}).get('timeout_seconds', 30)
    # Global timeout should be much longer than individual evaluation timeout
    global_timeout = max(task_timeout * 10, 300)  # At least 5 minutes
    
    def train_task():
        trainer.train(model=param, algorithm=algo_cls, **params)  # runs and mutates `param`
        return param
    
    try:
        # Use timeout wrapper to prevent hanging
        param = run_with_timeout(train_task, global_timeout)
        elapsed = time.time() - start
    except TimeoutError as e:
        elapsed = time.time() - start
        print(f"    Training timed out after {global_timeout}s")
        # Return current state with timeout indicator
        final_code = getattr(param, 'data', None)
        score, fb = guide('', final_code or initial_params, ds['infos'][0])
        return (float(score) if score is not None else float('-inf')), elapsed, dict(
            feedback=f"Training timed out: {str(e)}",
            initial_params=initial_params,
            final_params=final_code,
            log_dir=log_dir,
            timestamp=timestamp,
            timeout_occurred=True
        )

    # Evaluate final parameter directly via the guide on one sample (same as ds)
    final_code = getattr(param, 'data', None)
    score, fb = guide('', final_code, ds['infos'][0])
    
    return (float(score) if score is not None else float('-inf')), elapsed, dict(
        feedback=fb,
        initial_params=initial_params,
        final_params=final_code,
        log_dir=log_dir,
        timestamp=timestamp
    )

def main():
    ap = argparse.ArgumentParser(description='Run Trace trainers on benchmark LLM4AD tasks.')
    ap.add_argument('--tasks', type=str, required=True, help='Folder with benchmark task directories')
    ap.add_argument('--task', type=str, required=True, help='Task key(s) (e.g., "circle_packing" or "circle_packing,acrobot,knapsack" for multiple tasks)')
    ap.add_argument('--algos', type=str, default='PrioritySearch', help='Comma-separated algorithms: PrioritySearch,GEPA-Base,GEPA-UCB,GEPA-Beam')
    ap.add_argument('--threads', type=int, default=2, help='Num threads used by algorithms')
    ap.add_argument('--optimizer-kwargs', type=str, default='', help='JSON dict to merge into optimizer_kwargs')
    ap.add_argument('--eval-kwargs', type=str, default='', help='JSON dict passed into the evaluator ctor')
    # Some knobs
    ap.add_argument('--gepa-iters', type=int, default=3)
    ap.add_argument('--gepa-train-bs', type=int, default=2)
    ap.add_argument('--gepa-merge-every', type=int, default=2)
    ap.add_argument('--gepa-pareto-subset', type=int, default=3)
    ap.add_argument('--ps-steps', type=int, default=3)
    ap.add_argument('--ps-batches', type=int, default=2)
    ap.add_argument('--ps-candidates', type=int, default=3)
    ap.add_argument('--ps-proposals', type=int, default=3)
    ap.add_argument('--ps-mem-update', type=int, default=2)
    args = ap.parse_args()

    tasks_dir = Path(args.tasks).resolve()
    algo_names = [s.strip() for s in args.algos.split(',') if s.strip()]
    algo_map = {
        'PrioritySearch': SearchAlgorithm,
        'GEPA-Base': GEPAAlgorithmBase,
        'GEPA-UCB': GEPAUCBSearch,
        'GEPA-Beam': GEPABeamPareto,
    }

    extra_opt = json.loads(args.optimizer_kwargs) if args.optimizer_kwargs else {}
    eval_kwargs = json.loads(args.eval_kwargs) if args.eval_kwargs else {}

    # Parse multiple tasks
    task_keys = [key.strip() for key in args.task.split(',') if key.strip()]
    
    trainer_overrides = dict(
        eval_kwargs=eval_kwargs,
        gepa_iters=args.gepa_iters,
        gepa_train_bs=args.gepa_train_bs,
        gepa_merge_every=args.gepa_merge_every,
        gepa_pareto_subset=args.gepa_pareto_subset,
        ps_steps=args.ps_steps,
        ps_batches=args.ps_batches,
        ps_candidates=args.ps_candidates,
        ps_proposals=args.ps_proposals,
        ps_mem_update=args.ps_mem_update,
    )

    all_results = []

    for task_key in task_keys:
        print(f"\n{'='*60}")
        print(f"PROCESSING TASK: {task_key}")
        print(f"{'='*60}")
        
        try:
            task_dir = pick_benchmark_task(tasks_dir, task_key)
            mod = load_benchmark_task(task_dir)
        except Exception as e:
            print(f"Failed to load task {task_key}: {e}")
            continue
        
        task_results = []
        
        for name in algo_names:
            if name not in algo_map:
                print(f'[SKIP] Unknown algo: {name}')
                continue
            algo_cls = algo_map[name]
            print(f"\n=== Running {name} on benchmark task '{task_key}' ===")
            try:
                score, secs, meta = run_one(mod, name, algo_cls, threads=args.threads, optimizer_kwargs=extra_opt, trainer_overrides=trainer_overrides, task_name=task_key)
                print(f"{name}: score={score:.4f}  time={secs:.2f}s")
                result = dict(task=task_key, algo=name, score=float(score), time=float(secs), meta=meta)
                task_results.append(result)
                all_results.append(result)
            except Exception as e:
                print(f"Error running {name} on {task_key}: {e}")
                result = dict(task=task_key, algo=name, score=float('-inf'), time=0.0, meta=dict(error=str(e)))
                task_results.append(result)
                all_results.append(result)
        
        # Task summary
        print(f"\n--- TASK {task_key} SUMMARY ---")
        for r in task_results:
            if 'error' not in r['meta']:
                print(f"{r['algo']:>12} | score={r['score']:.4f} | time={r['time']:.2f}s")
            else:
                print(f"{r['algo']:>12} | ERROR: {r['meta']['error'][:50]}...")
    
    results = all_results  # Use all_results for final CSV output

    # Overall Summary
    print('\n========== OVERALL SUMMARY ==========')
    for r in results:
        if 'error' not in r['meta']:
            print(f"{r['task']:>20} | {r['algo']:>12} | score={r['score']:.4f} | time={r['time']:.2f}s")
        else:
            print(f"{r['task']:>20} | {r['algo']:>12} | ERROR")
    
    # CSV Logging
    csv_filename = f"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    os.makedirs('./results', exist_ok=True)
    csv_path = f'./results/{csv_filename}'
    
    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['timestamp', 'task', 'algo', 'parameters', 'time', 'score', 'initial_params', 'final_params', 'log_dir']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        for r in results:
            # Convert parameters to a single line string
            params_str = json.dumps(trainer_overrides, separators=(',', ':')).replace('\\n', '\\\\n')
            initial_params_str = str(r['meta'].get('initial_params', '')).replace('\\n', '\\\\n')
            final_params_str = str(r['meta'].get('final_params', '')).replace('\\n', '\\\\n')
            
            writer.writerow({
                'timestamp': r['meta'].get('timestamp', ''),
                'task': r.get('task', args.task),
                'algo': r['algo'],
                'parameters': params_str,
                'time': r['time'],
                'score': r['score'],
                'initial_params': initial_params_str,
                'final_params': final_params_str,
                'log_dir': r['meta'].get('log_dir', '')
            })
    
    print(f"\\nResults saved to {csv_path}")
    print(f"TensorBoard logs saved to ./logs/{args.task}/")


if __name__ == '__main__':
    main()