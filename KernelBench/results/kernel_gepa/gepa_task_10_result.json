{
  "task_idx": 10,
  "best_speedup": 1.0076335877862594,
  "num_metric_calls": 23,
  "best_at_metric_call": 18,
  "best_cuda_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define CUDA kernel for optimized matrix multiplication\nmatrix_mul_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <c10/cuda/CUDAStream.h>\n#include <cublas_v2.h>\n\n// Persistent cuBLAS handle singleton to avoid create/destroy overhead\nstatic cublasHandle_t& get_cublas_handle() {\n    static cublasHandle_t handle = nullptr;\n    static bool initialized = false;\n    \n    if (!initialized) {\n        cublasCreate(&handle);\n        initialized = true;\n    }\n    return handle;\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Ensure the tensors are on CUDA\n    A = A.contiguous().cuda();\n    B = B.contiguous().cuda();\n    \n    // Get the dimensions\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    \n    // Create output tensor\n    auto options = torch::TensorOptions().dtype(A.dtype()).device(A.device());\n    auto C = torch::empty({M, N}, options);\n    \n    // Get cublas handle\n    cublasHandle_t& handle = get_cublas_handle();\n    \n    // Set stream\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n    cublasSetStream(handle, stream);\n    \n    // Perform matrix multiplication using cublas\n    // Use the appropriate precision based on input tensor type\n    if (A.scalar_type() == at::ScalarType::Float) {\n        float alpha = 1.0f;\n        float beta = 0.0f;\n        \n        // Using CUBLAS_OP_T for transposed operations can be more efficient\n        cublasSgemm(handle, \n                   CUBLAS_OP_N, CUBLAS_OP_N,\n                   N, M, K,\n                   &alpha,\n                   static_cast<float*>(B.data_ptr()), N,\n                   static_cast<float*>(A.data_ptr()), K,\n                   &beta,\n                   static_cast<float*>(C.data_ptr()), N);\n    } else if (A.scalar_type() == at::ScalarType::Half) {\n        // Support for half-precision\n        __half alpha = __float2half(1.0f);\n        __half beta = __float2half(0.0f);\n        \n        cublasHgemm(handle,\n                   CUBLAS_OP_N, CUBLAS_OP_N,\n                   N, M, K,\n                   &alpha,\n                   static_cast<__half*>(B.data_ptr()), N,\n                   static_cast<__half*>(A.data_ptr()), K,\n                   &beta,\n                   static_cast<__half*>(C.data_ptr()), N);\n    }\n    \n    return C;\n}\n\"\"\"\n\nmatrix_mul_cpp_source = \"\"\"\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\n# Compile the inline CUDA code for matrix multiplication\nmatrix_mul = load_inline(\n    name=\"matrix_mul\",\n    cpp_sources=matrix_mul_cpp_source,\n    cuda_sources=matrix_mul_source,\n    functions=[\"matmul_cuda\"],\n    verbose=True,\n    extra_cflags=[\"-O3\"],\n    with_cuda=True\n)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that performs a single matrix multiplication (C = A * B)\n    using a custom CUDA kernel with cuBLAS\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matrix_mul = matrix_mul\n        \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs optimized matrix multiplication using cuBLAS.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        return self.matrix_mul.matmul_cuda(A, B)\n\ndef get_inputs():\n    M = 1024 * 2\n    K = 4096 * 2\n    N = 2048 * 2\n    A = torch.rand(M, K).cuda()\n    B = torch.rand(K, N).cuda()\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
  "num_candidates": 3,
  "duration_seconds": 919.7407236099243,
  "val_aggregate_scores": [
    0.0,
    0.9498207885304659,
    1.0076335877862594
  ],
  "settings": {
    "reflection_lm": "anthropic/claude-3.7-sonnet",
    "max_metric_calls": 10,
    "reflection_minibatch_size": 1,
    "num_correct_trials": 1,
    "num_perf_trials": 5
  }
}