{
  "task_idx": 0,
  "best_speedup": 1.7696160267111856,
  "num_metric_calls": 23,
  "best_at_metric_call": 6,
  "best_cuda_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for tensor-matrix multiplication\nmatmul_3d_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cublas_v2.h>\n\ntorch::Tensor batched_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Get dimensions\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n    \n    // Ensure B is on the correct device\n    B = B.to(A.device());\n    \n    // Create output tensor\n    auto options = torch::TensorOptions()\n        .dtype(A.dtype())\n        .device(A.device());\n    auto C = torch::empty({N, M, L}, options);\n    \n    // Get raw pointers\n    float* a_ptr = A.data_ptr<float>();\n    float* b_ptr = B.data_ptr<float>();\n    float* c_ptr = C.data_ptr<float>();\n    \n    // Create cuBLAS handle\n    cublasHandle_t handle;\n    cublasCreate(&handle);\n    cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH);\n    \n    // Set up scaling factors\n    float alpha = 1.0f;\n    float beta = 0.0f;\n    \n    // Use batch strided version instead of a loop\n    // This allows cuBLAS to optimize across batches\n    long long int strideA = M * K;\n    long long int strideC = M * L;\n    \n    cublasSgemmStridedBatched(\n        handle,\n        CUBLAS_OP_N, CUBLAS_OP_N,      // No transposition\n        L, M, K,                       // dimensions (m, n, k)\n        &alpha,                        // alpha\n        b_ptr, L,                      // B, ldb\n        0,                             // stride for B (not batched)\n        a_ptr, K,                      // A, lda\n        strideA,                       // stride for A between batches\n        &beta,                         // beta\n        c_ptr, L,                      // C, ldc\n        strideC,                       // stride for C between batches\n        N                              // batch count\n    );\n    \n    // Destroy handle\n    cublasDestroy(handle);\n    \n    return C;\n}\n\"\"\"\n\nmatmul_3d_cpp_source = \"\"\"\ntorch::Tensor batched_matmul_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\n# Compile the inline CUDA code\nmatmul_3d = load_inline(\n    name=\"matmul_3d\",\n    cpp_sources=matmul_3d_cpp_source,\n    cuda_sources=matmul_3d_source,\n    functions=[\"batched_matmul_cuda\"],\n    verbose=True,\n    extra_cflags=[\"-O3\"],\n    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\", \"-Xptxas=-v\"],\n    extra_ldflags=[\"-lcublas\"],\n)\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication with optimized CUDA kernel.\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matmul_3d = matmul_3d\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs 3D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 3D tensor of shape (N, M, K).\n            B (torch.Tensor): Input matrix of shape (K, L).\n\n        Returns:\n            torch::Tensor: Output tensor of shape (N, M, L), resulting from the multiplication of A and B.\n        \"\"\"\n        # Use PyTorch's implementation for CPU tensors\n        if not A.is_cuda or not B.is_cuda:\n            return torch.matmul(A, B)\n            \n        # Use our custom kernel for CUDA tensors\n        return self.matmul_3d.batched_matmul_cuda(A, B)\n\n\ndef get_inputs():\n    A = torch.rand(16, 1024, 2048).cuda()\n    B = torch.rand(2048, 768).cuda()\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
  "num_candidates": 3,
  "duration_seconds": 913.3866052627563,
  "val_aggregate_scores": [
    0.0,
    0.592391304347826,
    1.7696160267111856
  ],
  "settings": {
    "reflection_lm": "anthropic/claude-3.7-sonnet",
    "max_metric_calls": 10,
    "reflection_minibatch_size": 1,
    "num_correct_trials": 1,
    "num_perf_trials": 5
  }
}