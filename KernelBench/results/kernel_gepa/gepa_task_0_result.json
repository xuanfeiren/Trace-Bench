{
  "task_idx": 0,
  "best_speedup": 0.7482517482517483,
  "num_metric_calls": 11,
  "best_at_metric_call": 6,
  "best_cuda_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the improved custom CUDA kernel for 3D tensor-matrix multiplication\nmatmul3d_cuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cublas_v2.h>\n\n// CUDA kernel for 3D tensor-matrix multiplication using cuBLAS batched GEMM\ntorch::Tensor matmul3d_cuda(torch::Tensor A, torch::Tensor B) {\n    // Get shapes\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n    \n    // Create output tensor\n    auto options = torch::TensorOptions()\n        .dtype(A.dtype())\n        .device(A.device());\n    auto C = torch::empty({N, M, L}, options);\n    \n    // Get pointers to data\n    float* A_data = A.data_ptr<float>();\n    float* B_data = B.data_ptr<float>();\n    float* C_data = C.data_ptr<float>();\n    \n    // Initialize cuBLAS\n    cublasHandle_t handle;\n    cublasCreate(&handle);\n    \n    // cuBLAS constants\n    float alpha = 1.0f;\n    float beta = 0.0f;\n    \n    // Create arrays of pointers for batched GEMM\n    float** A_array = new float*[N];\n    float** C_array = new float*[N];\n    \n    // Setup pointer arrays for batch processing\n    for (int n = 0; n < N; n++) {\n        A_array[n] = A_data + n * M * K;\n        C_array[n] = C_data + n * M * L;\n    }\n    \n    // Transfer pointer arrays to device memory\n    float** d_A_array;\n    float** d_C_array;\n    cudaMalloc((void**)&d_A_array, N * sizeof(float*));\n    cudaMalloc((void**)&d_C_array, N * sizeof(float*));\n    cudaMemcpy(d_A_array, A_array, N * sizeof(float*), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_C_array, C_array, N * sizeof(float*), cudaMemcpyHostToDevice);\n    \n    // Perform batched matrix multiplication using cublasSgemmBatched\n    // Note: cuBLAS uses column-major order\n    cublasSgemmStridedBatched(\n        handle,\n        CUBLAS_OP_N,    // Operation on B\n        CUBLAS_OP_N,    // Operation on A\n        L,              // Number of rows of B and C\n        M,              // Number of columns of A and C\n        K,              // Number of columns of B and rows of A\n        &alpha,\n        B_data,         // B\n        L,              // Leading dimension of B\n        0,              // Stride between batches of B (we reuse B for all batches)\n        A_data,         // A\n        K,              // Leading dimension of A\n        M * K,          // Stride between batches of A\n        &beta,\n        C_data,         // C\n        L,              // Leading dimension of C\n        M * L,          // Stride between batches of C\n        N               // Batch count\n    );\n    \n    // Clean up\n    cublasDestroy(handle);\n    delete[] A_array;\n    delete[] C_array;\n    cudaFree(d_A_array);\n    cudaFree(d_C_array);\n    \n    return C;\n}\n\"\"\"\n\nmatmul3d_cpp_source = \"\"\"\ntorch::Tensor matmul3d_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\n# Compile the custom CUDA kernel\nmatmul3d_op = load_inline(\n    name=\"matmul3d_op\",\n    cpp_sources=matmul3d_cpp_source,\n    cuda_sources=matmul3d_cuda_source,\n    functions=[\"matmul3d_cuda\"],\n    verbose=True,\n    with_cuda=True,\n    extra_cuda_cflags=[\"-lcublas\"],\n)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication using a custom CUDA kernel.\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matmul3d_op = matmul3d_op\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs 3D tensor-matrix multiplication with custom CUDA implementation.\n\n        Args:\n            A (torch.Tensor): Input 3D tensor of shape (N, M, K).\n            B (torch.Tensor): Input matrix of shape (K, L).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (N, M, L).\n        \"\"\"\n        # Make sure inputs are on the GPU\n        A = A.cuda() if not A.is_cuda else A\n        B = B.cuda() if not B.is_cuda else B\n        \n        # Use our custom CUDA kernel\n        return self.matmul3d_op.matmul3d_cuda(A, B)",
  "num_candidates": 3,
  "duration_seconds": 314.25795888900757,
  "val_aggregate_scores": [
    0.0,
    0.5837837837837838,
    0.7482517482517483
  ],
  "settings": {
    "reflection_lm": "anthropic/claude-3.7-sonnet",
    "max_metric_calls": 10,
    "reflection_minibatch_size": 1,
    "num_correct_trials": 1,
    "num_perf_trials": 5
  }
}