{
  "task_idx": 13,
  "best_speedup": 0.3644067796610169,
  "num_metric_calls": 27,
  "best_at_metric_call": 26,
  "best_cuda_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define optimized CUDA kernel for matrix multiplication with small K dimension\nmatmul_cuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <mma.h>\n\n// Using shared memory and vectorized loads for improved performance\n__global__ void matmul_small_k_kernel(\n    const float* __restrict__ A,\n    const float* __restrict__ B,\n    float* __restrict__ C,\n    const int M, const int N, const int K) {\n    \n    // Tile sizes for shared memory optimization\n    constexpr int TILE_SIZE_M = 64;  \n    constexpr int TILE_SIZE_N = 64;\n    constexpr int TILE_SIZE_K = 8;   // K is small, so we use a smaller tile\n    \n    // Shared memory for input tiles\n    __shared__ float As[TILE_SIZE_M][TILE_SIZE_K];\n    __shared__ float Bs[TILE_SIZE_K][TILE_SIZE_N];\n    \n    // Block indices\n    const int bx = blockIdx.x;\n    const int by = blockIdx.y;\n    \n    // Thread indices\n    const int tx = threadIdx.x;\n    const int ty = threadIdx.y;\n    \n    // Thread ID in the block\n    const int tid = ty * blockDim.x + tx;\n    \n    // Each thread computes multiple output elements\n    float accum[4][4] = {0};  // 4x4 block per thread\n    \n    // Starting indices for this thread block\n    const int block_row_start = by * TILE_SIZE_M;\n    const int block_col_start = bx * TILE_SIZE_N;\n    \n    // Load and process K-dimension tiles\n    for (int k_tile = 0; k_tile < (K + TILE_SIZE_K - 1) / TILE_SIZE_K; ++k_tile) {\n        // Collaborative loading of A and B tiles into shared memory\n        // Using thread coalescing for efficient memory access\n        const int k_start = k_tile * TILE_SIZE_K;\n        \n        // Each thread loads multiple elements\n        for (int i = tid; i < TILE_SIZE_M * TILE_SIZE_K; i += blockDim.x * blockDim.y) {\n            int tile_row = i / TILE_SIZE_K;\n            int tile_col = i % TILE_SIZE_K;\n            int global_row = block_row_start + tile_row;\n            int global_col = k_start + tile_col;\n            \n            if (global_row < M && global_col < K) {\n                As[tile_row][tile_col] = A[global_row * K + global_col];\n            } else {\n                As[tile_row][tile_col] = 0.0f;\n            }\n        }\n        \n        for (int i = tid; i < TILE_SIZE_K * TILE_SIZE_N; i += blockDim.x * blockDim.y) {\n            int tile_row = i / TILE_SIZE_N;\n            int tile_col = i % TILE_SIZE_N;\n            int global_row = k_start + tile_row;\n            int global_col = block_col_start + tile_col;\n            \n            if (global_row < K && global_col < N) {\n                Bs[tile_row][tile_col] = B[global_row * N + global_col];\n            } else {\n                Bs[tile_row][tile_col] = 0.0f;\n            }\n        }\n        \n        // Synchronize to make sure the tiles are loaded\n        __syncthreads();\n        \n        // Each thread computes multiple output elements (4x4 block)\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE_K; ++k) {\n            #pragma unroll\n            for (int i = 0; i < 4; ++i) {\n                int local_row = ty * 4 + i;\n                if (local_row < TILE_SIZE_M) {\n                    float a_val = As[local_row][k];\n                    #pragma unroll\n                    for (int j = 0; j < 4; ++j) {\n                        int local_col = tx * 4 + j;\n                        if (local_col < TILE_SIZE_N) {\n                            accum[i][j] += a_val * Bs[k][local_col];\n                        }\n                    }\n                }\n            }\n        }\n        \n        // Synchronize before loading the next tile\n        __syncthreads();\n    }\n    \n    // Write results back to global memory\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int global_row = block_row_start + ty * 4 + i;\n        if (global_row < M) {\n            #pragma unroll\n            for (int j = 0; j < 4; ++j) {\n                int global_col = block_col_start + tx * 4 + j;\n                if (global_col < N) {\n                    C[global_row * N + global_col] = accum[i][j];\n                }\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Get tensor dimensions\n    const int M = A.size(0);\n    const int K = A.size(1);\n    const int N = B.size(1);\n    \n    // Create output tensor\n    auto options = torch::TensorOptions()\n        .dtype(A.dtype())\n        .device(A.device());\n    auto C = torch::zeros({M, N}, options);\n    \n    // Configure kernel launch parameters\n    // Each thread handles a 4x4 block of output elements\n    const int TILE_SIZE_M = 64;\n    const int TILE_SIZE_N = 64;\n    \n    dim3 threads(16, 16);  // 16x16 = 256 threads per block\n    dim3 grid(\n        (N + TILE_SIZE_N - 1) / TILE_SIZE_N, \n        (M + TILE_SIZE_M - 1) / TILE_SIZE_M\n    );\n    \n    // Launch the kernel\n    matmul_small_k_kernel<<<grid, threads>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, N, K\n    );\n    \n    return C;\n}\n\"\"\"\n\nmatmul_cpp_source = \"\"\"\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\n# Compile the inline CUDA code\nmatmul_extension = load_inline(\n    name=\"matmul_cuda\",\n    cpp_sources=matmul_cpp_source,\n    cuda_sources=matmul_cuda_source,\n    functions=[\"matmul_cuda\"],\n    verbose=True,\n    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"]  # Enhanced optimizations\n)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that performs matrix multiplication using a custom CUDA kernel\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matmul_extension = matmul_extension\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication using custom CUDA kernel.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        # Make sure inputs are on the GPU\n        A = A.cuda() if not A.is_cuda else A\n        B = B.cuda() if not B.is_cuda else B\n        \n        # Convert to float32 if needed\n        if A.dtype != torch.float32:\n            A = A.to(torch.float32)\n        if B.dtype != torch.float32:\n            B = B.to(torch.float32)\n            \n        return self.matmul_extension.matmul_cuda(A, B)\n\n# Keep the original functions\nM = 16384 * 2\nN = 16384 * 2\nK = 32 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K)\n    B = torch.rand(K, N)\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
  "num_candidates": 7,
  "duration_seconds": 1182.431880235672,
  "val_aggregate_scores": [
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.0,
    0.3644067796610169
  ],
  "settings": {
    "reflection_lm": "anthropic/claude-3.7-sonnet",
    "max_metric_calls": 10,
    "reflection_minibatch_size": 1,
    "num_correct_trials": 1,
    "num_perf_trials": 5
  }
}