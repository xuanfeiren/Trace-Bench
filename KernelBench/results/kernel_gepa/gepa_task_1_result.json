{
  "task_idx": 1,
  "best_speedup": 1.0619469026548671,
  "num_metric_calls": 24,
  "best_at_metric_call": 17,
  "best_cuda_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for 4D tensor-matrix multiplication\ntensor_matmul_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <ATen/cuda/CUDABlas.h>\n#include <cublas_v2.h>\n\n// Custom implementation using cuBLAS with optimized memory layout and batching\ntorch::Tensor tensor_matmul_cublas_optimized(torch::Tensor A, torch::Tensor B) {\n    // Get tensor dimensions\n    int b = A.size(0);\n    int i = A.size(1);\n    int j = A.size(2);\n    int l = A.size(3);\n    int k = B.size(1);\n    \n    // Reshape A to [b*i*j, l] for efficient batched matmul\n    auto A_2d = A.reshape({b*i*j, l}).contiguous();\n    \n    // Make sure B is contiguous for optimal memory access\n    auto B_cont = B.contiguous();\n    \n    // Prepare output tensor with optimal memory layout\n    auto options = torch::TensorOptions().dtype(A.dtype()).device(A.device());\n    auto C_2d = torch::empty({b*i*j, k}, options);\n    \n    // Get CUDA stream\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n    \n    // Get cuBLAS handle\n    cublasHandle_t handle = at::cuda::getCurrentCUDABlasHandle();\n    \n    // Set stream for cuBLAS operations\n    cublasSetStream(handle, stream);\n    \n    // Get pointers for cuBLAS call\n    float* A_ptr = A_2d.data_ptr<float>();\n    float* B_ptr = B_cont.data_ptr<float>();\n    float* C_ptr = C_2d.data_ptr<float>();\n    \n    // Scale factors\n    float alpha = 1.0f;\n    float beta = 0.0f;\n    \n    // Perform matrix multiplication with cuBLAS\n    // C = alpha * A * B + beta * C\n    // Note: cuBLAS uses column-major order, so we compute B^T * A^T = (A * B)^T\n    // by swapping the order of matrices and transposing the result\n    cublasSgemm(handle,\n               CUBLAS_OP_N,       // B is not transposed\n               CUBLAS_OP_N,       // A is not transposed\n               k,                 // Rows of B matrix (output columns)\n               b*i*j,             // Columns of A matrix (output rows) \n               l,                 // Cols of A / Rows of B (inner dimension)\n               &alpha,            // Scaling factor for multiplication\n               B_ptr,             // B matrix data\n               k,                 // Leading dimension of B\n               A_ptr,             // A matrix data\n               l,                 // Leading dimension of A\n               &beta,             // Scaling factor for C\n               C_ptr,             // C matrix data\n               k);                // Leading dimension of C\n    \n    // Reshape the result back to [b, i, j, k]\n    auto C = C_2d.reshape({b, i, j, k});\n    \n    return C;\n}\n\n// Alternative implementation using tensor cores if available\ntorch::Tensor tensor_matmul_tensorcore(torch::Tensor A, torch::Tensor B) {\n    // Get tensor dimensions\n    int b = A.size(0);\n    int i = A.size(1);\n    int j = A.size(2);\n    int l = A.size(3);\n    int k = B.size(1);\n    \n    // Convert to half precision for tensor cores if input is already float16\n    bool use_half = (A.dtype() == torch::kHalf);\n    \n    if (use_half) {\n        // Reshape A to [b*i*j, l]\n        auto A_2d = A.reshape({b*i*j, l});\n        \n        // Perform batch matrix multiplication with tensor cores\n        auto C_2d = torch::matmul(A_2d, B);\n        \n        // Reshape the result back to [b, i, j, k]\n        auto C = C_2d.reshape({b, i, j, k});\n        return C;\n    }\n    else {\n        // Fall back to optimized float32 implementation\n        return tensor_matmul_cublas_optimized(A, B);\n    }\n}\n\n// Smart dispatch based on tensor size and hardware\ntorch::Tensor tensor_matmul(torch::Tensor A, torch::Tensor B) {\n    // Check if the tensors are contiguous\n    if (!A.is_contiguous() || !B.is_contiguous()) {\n        A = A.contiguous();\n        B = B.contiguous();\n    }\n\n    // Get tensor dimensions\n    int b = A.size(0);\n    int i = A.size(1);\n    int j = A.size(2);\n    int l = A.size(3);\n    int k = B.size(1);\n    \n    // For very small matrices, PyTorch's einsum might be more efficient\n    if (b*i*j < 64 || l*k < 64) {\n        return torch::einsum(\"bijl,lk->bijk\", {A, B});\n    }\n    \n    // For all other cases, use our optimized implementation\n    return tensor_matmul_cublas_optimized(A, B);\n}\n\"\"\"\n\ntensor_matmul_cpp_source = \"\"\"\ntorch::Tensor tensor_matmul(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\n# Compile the inline CUDA code\ntensor_matmul_ext = load_inline(\n    name=\"tensor_matmul_ext\",\n    cpp_sources=tensor_matmul_cpp_source,\n    cuda_sources=tensor_matmul_source,\n    functions=[\"tensor_matmul\"],\n    verbose=True,\n    with_cuda=True,\n    extra_cflags=[\"-O3\"],\n    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\"]\n)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Performs 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n    \n    Using an optimized custom CUDA implementation\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.tensor_matmul_ext = tensor_matmul_ext\n\n    def forward(self, A, B):\n        \"\"\"\n        Performs the 4D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 4D tensor of shape (b, i, j, l)\n            B (torch.Tensor): Input matrix of shape (l, k)\n\n        Returns:\n            torch.Tensor: Output 4D tensor of shape (b, i, j, k)\n        \"\"\"\n        # Ensure tensors are on CUDA\n        if not A.is_cuda:\n            A = A.cuda()\n        if not B.is_cuda:\n            B = B.cuda()\n            \n        # Use our custom CUDA implementation\n        return self.tensor_matmul_ext.tensor_matmul(A, B)",
  "num_candidates": 4,
  "duration_seconds": 1047.817650794983,
  "val_aggregate_scores": [
    0.0,
    1.0,
    1.0169491525423728,
    1.0619469026548671
  ],
  "settings": {
    "reflection_lm": "anthropic/claude-3.7-sonnet",
    "max_metric_calls": 10,
    "reflection_minibatch_size": 1,
    "num_correct_trials": 1,
    "num_perf_trials": 5
  }
}