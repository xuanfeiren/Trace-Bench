{
  "task_idx": 11,
  "best_speedup": 1.017467248908297,
  "num_metric_calls": 22,
  "best_at_metric_call": 7,
  "best_cuda_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the CUDA kernel for batched matrix multiplication\nbatched_matmul_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cublas_v2.h>\n\n// Helper function to get the cublas handle\ncublasHandle_t get_cublas_handle() {\n    static cublasHandle_t handle = nullptr;\n    if (handle == nullptr) {\n        cublasCreate(&handle);\n    }\n    return handle;\n}\n\ntorch::Tensor batched_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Check that A and B are on the GPU\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    \n    // Check that A and B have the same batch size\n    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must have the same batch size\");\n    \n    // Check dimensions\n    TORCH_CHECK(A.size(2) == B.size(1), \"Inner dimensions must match for matrix multiplication\");\n    \n    // Get dimensions\n    int batch_size = A.size(0);\n    int m = A.size(1);\n    int k = A.size(2);\n    int n = B.size(2);\n    \n    // Create output tensor\n    auto C = torch::empty({batch_size, m, n}, \n                        torch::TensorOptions().dtype(A.dtype()).device(A.device()));\n    \n    // Get pointers to data\n    float *A_data = A.data_ptr<float>();\n    float *B_data = B.data_ptr<float>();\n    float *C_data = C.data_ptr<float>();\n    \n    // Get cuBLAS handle\n    cublasHandle_t handle = get_cublas_handle();\n    \n    // Scaling factors\n    float alpha = 1.0f;\n    float beta = 0.0f;\n    \n    // Stride information\n    long long int strideA = m * k;\n    long long int strideB = k * n;\n    long long int strideC = m * n;\n    \n    // Use cublasSgemmStridedBatched for batched matrix multiplication\n    // C = alpha * (A * B) + beta * C\n    cublasSgemmStridedBatched(\n        handle,                  // cuBLAS handle\n        CUBLAS_OP_N,             // A non-transposed\n        CUBLAS_OP_N,             // B non-transposed\n        n,                       // number of rows of matrix C and B\n        m,                       // number of columns of matrix C and A\n        k,                       // number of columns of B and rows of A\n        &alpha,                  // alpha scaling factor\n        B_data,                  // B array of pointers to matrices\n        n,                       // ldb\n        strideB,                 // stride between batches for B\n        A_data,                  // A array of pointers to matrices\n        k,                       // lda\n        strideA,                 // stride between batches for A\n        &beta,                   // beta scaling factor\n        C_data,                  // C array of pointers to matrices\n        n,                       // ldc\n        strideC,                 // stride between batches for C\n        batch_size               // batch size\n    );\n    \n    return C;\n}\n\"\"\"\n\nbatched_matmul_cpp_source = \"\"\"\ntorch::Tensor batched_matmul_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\n# Compile the inline CUDA code for batched matrix multiplication\nbatched_matmul = load_inline(\n    name=\"batched_matmul\",\n    cpp_sources=batched_matmul_cpp_source,\n    cuda_sources=batched_matmul_source,\n    functions=[\"batched_matmul_cuda\"],\n    verbose=True,\n    extra_cflags=[\"-O3\"],\n    extra_cuda_cflags=[\"-O3\", \"--use_fast_math\", \"-lcublas\"],\n    extra_ldflags=[\"-lcublas\"],\n)\n\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that performs batched matrix multiplication (C = A * B) \n    where A, B, and C have the same batch dimension.\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.batched_matmul = batched_matmul\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs batched matrix multiplication using a custom CUDA kernel.\n\n        Args:\n            A: Input tensor of shape (batch_size, m, k).\n            B: Input tensor of shape (batch_size, k, n).\n\n        Returns:\n            C: Output tensor of shape (batch_size, m, n).\n        \"\"\"\n        # Make sure tensors are on CUDA\n        A = A.cuda() if not A.is_cuda else A\n        B = B.cuda() if not B.is_cuda else B\n        \n        # Convert to float if not already\n        if A.dtype != torch.float32:\n            A = A.to(torch.float32)\n        if B.dtype != torch.float32:\n            B = B.to(torch.float32)\n            \n        return self.batched_matmul.batched_matmul_cuda(A, B)\n\n\nbatch_size = 128\nm = 128 * 4\nk = 256 * 4\nn = 512 * 4\n\ndef get_inputs():\n    A = torch.rand(batch_size, m, k).cuda()\n    B = torch.rand(batch_size, k, n).cuda()\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
  "num_candidates": 2,
  "duration_seconds": 885.4283182621002,
  "val_aggregate_scores": [
    0.0,
    1.017467248908297
  ],
  "settings": {
    "reflection_lm": "anthropic/claude-3.7-sonnet",
    "max_metric_calls": 10,
    "reflection_minibatch_size": 1,
    "num_correct_trials": 1,
    "num_perf_trials": 5
  }
}