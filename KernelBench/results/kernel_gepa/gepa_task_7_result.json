{
  "task_idx": 7,
  "best_speedup": 0.9692307692307692,
  "num_metric_calls": 22,
  "best_at_metric_call": 3,
  "best_cuda_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for matrix multiplication with transpose\nmatmul_trans_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cublas_v2.h>\n\ntorch::Tensor matmul_transpose_cuda(torch::Tensor A, torch::Tensor B) {\n    // Get dimensions\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(0);\n\n    // Create output tensor\n    auto options = torch::TensorOptions()\n        .dtype(A.dtype())\n        .device(A.device());\n    auto C = torch::empty({M, N}, options);\n\n    // Get pointers to data\n    float* a_ptr = A.data_ptr<float>();\n    float* b_ptr = B.data_ptr<float>();\n    float* c_ptr = C.data_ptr<float>();\n\n    // CUBLAS handle\n    cublasHandle_t handle;\n    cublasCreate(&handle);\n\n    // Set up CUBLAS for matrix multiplication\n    // Using GEMM: C = alpha*A*B + beta*C\n    // For A(M,K) * B(N,K)^T = C(M,N)\n    // In CUBLAS notation: C = alpha*op(A)*op(B) + beta*C\n    // We want: C = A * B^T\n    // In CUBLAS, matrices are column-major, but PyTorch is row-major\n    // So we compute B^T * A^T = (A * B^T)^T and then transpose the result\n    \n    float alpha = 1.0f;\n    float beta = 0.0f;\n    \n    // Perform the matrix multiplication: C = A * B^T\n    // Since CUBLAS uses column-major format and we have row-major:\n    // C^T = B * A^T  (in column-major CUBLAS notation)\n    // This translates to computing: C = A * B^T (in row-major PyTorch notation)\n    cublasSgemm(handle, \n                CUBLAS_OP_T,   // op(B) = B^T\n                CUBLAS_OP_N,   // op(A) = A\n                N,             // rows of op(B) = columns of result\n                M,             // columns of op(A) = rows of result\n                K,             // columns of op(B) = rows of op(A)\n                &alpha,\n                b_ptr,         // B matrix\n                K,             // leading dimension of B\n                a_ptr,         // A matrix\n                K,             // leading dimension of A\n                &beta, \n                c_ptr,         // C matrix\n                N);            // leading dimension of C\n    \n    // Destroy the handle\n    cublasDestroy(handle);\n    \n    return C;\n}\n\"\"\"\n\nmatmul_trans_cpp_source = \"\"\"\ntorch::Tensor matmul_transpose_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\n# Compile the inline CUDA code for matrix multiplication with transpose\nmatmul_transpose = load_inline(\n    name=\"matmul_transpose\",\n    cpp_sources=matmul_trans_cpp_source,\n    cuda_sources=matmul_trans_source,\n    functions=[\"matmul_transpose_cuda\"],\n    verbose=True,\n    extra_cuda_cflags=[\"-lcublas\"],\n    extra_ldflags=[\"-lcublas\"],\n)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that performs a single matrix multiplication (C = A * B)\n    using a custom CUDA kernel\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matmul_transpose = matmul_transpose\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs matrix multiplication using custom CUDA kernel.\n\n        Args:\n            A: Input tensor of shape (M, K).\n            B: Input tensor of shape (K, N).\n\n        Returns:\n            Output tensor of shape (M, N).\n        \"\"\"\n        # Ensure tensors are on GPU and convert to float32 if needed\n        if not A.is_cuda:\n            A = A.cuda()\n        if not B.is_cuda:\n            B = B.cuda()\n        \n        if A.dtype != torch.float32:\n            A = A.to(torch.float32)\n        if B.dtype != torch.float32:\n            B = B.to(torch.float32)\n            \n        return self.matmul_transpose.matmul_transpose_cuda(A, B)\n\n# Keep the original M, K, N values\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs():\n    A = torch.rand(M, K, device='cuda')\n    B = torch.rand(N, K, device='cuda')\n    return [A, B]\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed",
  "num_candidates": 2,
  "duration_seconds": 851.7262432575226,
  "val_aggregate_scores": [
    0.0,
    0.9692307692307692
  ],
  "settings": {
    "reflection_lm": "anthropic/claude-3.7-sonnet",
    "max_metric_calls": 10,
    "reflection_minibatch_size": 1,
    "num_correct_trials": 1,
    "num_perf_trials": 5
  }
}