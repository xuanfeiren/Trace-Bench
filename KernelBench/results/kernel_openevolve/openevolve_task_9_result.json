{
  "task_idx": 9,
  "best_speedup": 0.9190140845070423,
  "num_metric_calls": 10,
  "best_at_metric_call": 5,
  "best_cuda_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for matrix multiplication\nmatmul_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cublas_v2.h>\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Get dimensions\n    int m = A.size(0);\n    int n = B.size(1);\n    int k = A.size(1);\n    \n    // Create output tensor\n    auto C = torch::zeros({m, n}, A.options());\n    \n    // Get pointers to the data\n    const float* A_data = A.data_ptr<float>();\n    const float* B_data = B.data_ptr<float>();\n    float* C_data = C.data_ptr<float>();\n    \n    // Create cublas handle\n    cublasHandle_t handle;\n    cublasCreate(&handle);\n    \n    // Constants for cublasSgemm\n    float alpha = 1.0f;\n    float beta = 0.0f;\n    \n    // Perform matrix multiplication: C = alpha * A * B + beta * C\n    // Note: cublas uses column-major format, while PyTorch uses row-major\n    // So we compute B^T * A^T = (A * B)^T and then transpose the result\n    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, \n                n, m, k, \n                &alpha, \n                B_data, n,  // B transposed\n                A_data, k,  // A transposed\n                &beta, \n                C_data, n);\n    \n    // Destroy the handle\n    cublasDestroy(handle);\n    \n    // Return the result\n    return C;\n}\n\"\"\"\n\nmatmul_cpp_source = \"\"\"\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\n# Compile the inline CUDA code for matrix multiplication\nmatmul = load_inline(\n    name=\"matmul_cuda\",\n    cpp_sources=matmul_cpp_source,\n    cuda_sources=matmul_source,\n    functions=[\"matmul_cuda\"],\n    verbose=True,\n    extra_ldflags=[\"-lcublas\"],\n)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that performs a single square matrix multiplication (C = A * B)\n    using a custom CUDA kernel with cuBLAS\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.matmul = matmul\n    \n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication using custom CUDA kernel.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        # Make sure inputs are on CUDA\n        if not A.is_cuda:\n            A = A.cuda()\n        if not B.is_cuda:\n            B = B.cuda()\n            \n        return self.matmul.matmul_cuda(A, B)",
  "duration_seconds": 253.95168590545654,
  "metrics": {
    "combined_score": 0.9190140845070423
  },
  "settings": {
    "model": "claude-3.7-sonnet",
    "max_iterations": 10,
    "num_workers": 1,
    "evolution_strategy": "full_rewrite",
    "population_size": 50,
    "num_correct_trials": 1,
    "num_perf_trials": 5
  }
}