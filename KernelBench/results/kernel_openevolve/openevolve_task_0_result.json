{
  "task_idx": 0,
  "best_speedup": 0.767605633802817,
  "num_metric_calls": 10,
  "best_at_metric_call": 3,
  "best_cuda_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for 3D tensor-matrix multiplication\nbatched_matmul_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cublas_v2.h>\n\ntorch::Tensor batched_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Get dimensions\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n    \n    // Create output tensor\n    auto options = torch::TensorOptions().device(A.device()).dtype(A.dtype());\n    auto C = torch::empty({N, M, L}, options);\n    \n    // Get pointers to data\n    float* A_data = A.data_ptr<float>();\n    float* B_data = B.data_ptr<float>();\n    float* C_data = C.data_ptr<float>();\n    \n    // Create cuBLAS handle\n    cublasHandle_t handle;\n    cublasCreate(&handle);\n    \n    // Set up constants for GEMM\n    float alpha = 1.0f;\n    float beta = 0.0f;\n    \n    // Use cublasSgemmStridedBatched for more efficient batched matrix multiplication\n    cublasSgemmStridedBatched(\n        handle,\n        CUBLAS_OP_N, CUBLAS_OP_N,\n        L, M, K,\n        &alpha,\n        B_data, L, 0,\n        A_data, K, M*K,\n        &beta,\n        C_data, L, M*L,\n        N\n    );\n    \n    // Destroy cuBLAS handle\n    cublasDestroy(handle);\n    \n    return C;\n}\n\"\"\"\n\nbatched_matmul_cpp_source = \"\"\"\ntorch::Tensor batched_matmul_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\n# Compile the inline CUDA code\nbatched_matmul = load_inline(\n    name=\"batched_matmul\",\n    cpp_sources=batched_matmul_cpp_source,\n    cuda_sources=batched_matmul_source,\n    functions=[\"batched_matmul_cuda\"],\n    extra_cuda_cflags=[\"-lcublas\"],\n    with_cuda=True\n)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication using custom CUDA kernel.\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.batched_matmul = batched_matmul\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs 3D tensor-matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input 3D tensor of shape (N, M, K).\n            B (torch.Tensor): Input matrix of shape (K, L).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (N, M, L), resulting from the multiplication of A and B along the last dimension of A.\n        \"\"\"\n        # Move tensors to GPU if they're not already there\n        if not A.is_cuda:\n            A = A.cuda()\n        if not B.is_cuda:\n            B = B.cuda()\n            \n        return self.batched_matmul.batched_matmul_cuda(A, B)",
  "duration_seconds": 366.83962392807007,
  "metrics": {
    "combined_score": 0.767605633802817
  },
  "settings": {
    "model": "claude-3.7-sonnet",
    "max_iterations": 10,
    "num_workers": 1,
    "evolution_strategy": "full_rewrite",
    "population_size": 50,
    "num_correct_trials": 1,
    "num_perf_trials": 5
  }
}