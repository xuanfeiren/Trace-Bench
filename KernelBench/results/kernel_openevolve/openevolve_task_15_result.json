{
  "task_idx": 15,
  "best_speedup": 0.9207188160676533,
  "num_metric_calls": 11,
  "best_at_metric_call": 9,
  "best_cuda_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# Define the custom CUDA kernel for matrix multiplication optimized for tall-skinny matrices\nmatmul_cuda_source = \"\"\"\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cublas_v2.h>\n\ntorch::Tensor tall_skinny_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Get dimensions\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    \n    // Create output tensor\n    auto C = torch::empty({M, N}, A.options());\n    \n    // Get pointers to tensor data\n    const float* A_data = A.data_ptr<float>();\n    const float* B_data = B.data_ptr<float>();\n    float* C_data = C.data_ptr<float>();\n    \n    // Create cuBLAS handle\n    cublasHandle_t handle;\n    cublasCreate(&handle);\n    \n    // Set up alpha and beta for gemm\n    float alpha = 1.0f;\n    float beta = 0.0f;\n    \n    // Perform matrix multiplication using cuBLAS (which is optimized for GPU)\n    // Note: cuBLAS uses column-major format, so we compute (A*B) by calculating B^T * A^T\n    cublasSgemm(\n        handle,\n        CUBLAS_OP_N, CUBLAS_OP_N,\n        N, M, K,\n        &alpha,\n        B_data, N,\n        A_data, K,\n        &beta,\n        C_data, N\n    );\n    \n    // Destroy cuBLAS handle\n    cublasDestroy(handle);\n    \n    return C;\n}\n\ntorch::Tensor skinny_tall_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    // Get dimensions for the transposed case\n    int K = A.size(0);\n    int M = A.size(1);\n    int N = B.size(0);\n    \n    // Create output tensor\n    auto C = torch::empty({N, M}, A.options());\n    \n    // Get pointers to tensor data\n    const float* A_data = A.data_ptr<float>();\n    const float* B_data = B.data_ptr<float>();\n    float* C_data = C.data_ptr<float>();\n    \n    // Create cuBLAS handle\n    cublasHandle_t handle;\n    cublasCreate(&handle);\n    \n    // Set up alpha and beta for gemm\n    float alpha = 1.0f;\n    float beta = 0.0f;\n    \n    // For the transposed case\n    cublasSgemm(\n        handle,\n        CUBLAS_OP_T, CUBLAS_OP_T,\n        M, N, K,\n        &alpha,\n        A_data, K,\n        B_data, N,\n        &beta,\n        C_data, M\n    );\n    \n    // Destroy cuBLAS handle\n    cublasDestroy(handle);\n    \n    return C;\n}\n\"\"\"\n\nmatmul_cpp_source = \"\"\"\ntorch::Tensor tall_skinny_matmul_cuda(torch::Tensor A, torch::Tensor B);\ntorch::Tensor skinny_tall_matmul_cuda(torch::Tensor A, torch::Tensor B);\n\"\"\"\n\n# Compile the custom CUDA kernel\nmatmul_cuda = load_inline(\n    name=\"matmul_cuda\",\n    cpp_sources=matmul_cpp_source,\n    cuda_sources=matmul_cuda_source,\n    functions=[\"tall_skinny_matmul_cuda\", \"skinny_tall_matmul_cuda\"],\n    with_cuda=True,\n    extra_ldflags=[\"-lcublas\"]\n)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Optimized model that performs a single matrix multiplication (C = A * B) \n    where one of the matrices is tall and skinny (M >> N or N >> M)\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n    \n    def forward(self, A, B):\n        \"\"\"\n        Performs the matrix multiplication with custom CUDA kernel.\n\n        Args:\n            A (torch.Tensor): Input matrix of shape (M, K) or (K, M) where M >> N or N >> M.\n            B (torch.Tensor): Input matrix of shape (K, N) or (N, K) where M >> N or N >> M.\n\n        Returns:\n            torch.Tensor: Output matrix of shape (M, N) or (N, M)\n        \"\"\"\n        # Ensure tensors are on GPU\n        A = A.cuda() if not A.is_cuda else A\n        B = B.cuda() if not B.is_cuda else B\n        \n        # Make sure we're working with contiguous tensors\n        A = A.contiguous() if not A.is_contiguous() else A\n        B = B.contiguous() if not B.is_contiguous() else B\n        \n        # Choose appropriate kernel based on input dimensions\n        if A.size(0) > A.size(1) and B.size(0) == A.size(1):\n            # A is (M,K), B is (K,N) where M >> K\n            return matmul_cuda.tall_skinny_matmul_cuda(A, B)\n        elif A.size(0) < A.size(1) and B.size(1) == A.size(0):\n            # A is (K,M), B is (N,K) where M >> N\n            return matmul_cuda.skinny_tall_matmul_cuda(A, B)\n        else:\n            # Fall back to PyTorch's implementation for other cases\n            return torch.matmul(A, B)",
  "duration_seconds": 447.1693742275238,
  "history": [
    {
      "attempt": 1,
      "score": 0.0,
      "best_score": 0.0,
      "is_new_best": false,
      "iteration_found": 0
    },
    {
      "attempt": 2,
      "score": 0.0,
      "best_score": 0.0,
      "is_new_best": false,
      "iteration_found": 1
    },
    {
      "attempt": 3,
      "score": 0.0,
      "best_score": 0.0,
      "is_new_best": false,
      "iteration_found": 2
    },
    {
      "attempt": 4,
      "score": 0.0,
      "best_score": 0.0,
      "is_new_best": false,
      "iteration_found": 3
    },
    {
      "attempt": 5,
      "score": 0.0,
      "best_score": 0.0,
      "is_new_best": false,
      "iteration_found": 4
    },
    {
      "attempt": 6,
      "score": 0.8494736842105264,
      "best_score": 0.8494736842105264,
      "is_new_best": true,
      "iteration_found": 5
    },
    {
      "attempt": 7,
      "score": 0.8572938689217757,
      "best_score": 0.8572938689217757,
      "is_new_best": true,
      "iteration_found": 6
    },
    {
      "attempt": 8,
      "score": 0.0,
      "best_score": 0.8572938689217757,
      "is_new_best": false,
      "iteration_found": 7
    },
    {
      "attempt": 9,
      "score": 0.0,
      "best_score": 0.8572938689217757,
      "is_new_best": false,
      "iteration_found": 8
    },
    {
      "attempt": 10,
      "score": 0.9207188160676533,
      "best_score": 0.9207188160676533,
      "is_new_best": true,
      "iteration_found": 9
    },
    {
      "attempt": 11,
      "score": 0.8551797040169132,
      "best_score": 0.9207188160676533,
      "is_new_best": false,
      "iteration_found": 10
    }
  ],
  "metrics": {
    "combined_score": 0.9207188160676533
  },
  "settings": {
    "model": "claude-3.7-sonnet",
    "max_iterations": 10,
    "num_workers": 1,
    "evolution_strategy": "full_rewrite",
    "population_size": 50,
    "num_correct_trials": 1,
    "num_perf_trials": 5
  }
}