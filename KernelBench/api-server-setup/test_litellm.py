#!/usr/bin/env python3
"""
æµ‹è¯• LiteLLM è°ƒç”¨ Claude çš„è„šæœ¬
ä½¿ç”¨æ–¹æ³•ï¼špython test_litellm.py
"""

import os
from litellm import completion

# å¦‚æœä½¿ç”¨ LiteLLM Proxy Serverï¼Œè®¾ç½® API base
# os.environ["LITELLM_API_BASE"] = "http://localhost:4000"
# os.environ["LITELLM_API_KEY"] = "sk-1234"

def test_direct_bedrock():
    """ç›´æ¥é€šè¿‡ LiteLLM è°ƒç”¨ Bedrock"""
    print("ğŸ§ª æµ‹è¯• 1: ç›´æ¥é€šè¿‡ LiteLLM è°ƒç”¨ Bedrock Claude")
    print("-" * 50)
    
    try:
        response = completion(
            model="bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0",
            messages=[
                {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»é‡å­è®¡ç®—"}
            ],
            aws_region_name="us-east-1",
            max_tokens=200
        )
        
        print("âœ… è°ƒç”¨æˆåŠŸï¼")
        print(f"å›å¤: {response.choices[0].message.content}")
        print()
        
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")
        print()


def test_litellm_proxy():
    """é€šè¿‡ LiteLLM Proxy Server è°ƒç”¨"""
    print("ğŸ§ª æµ‹è¯• 2: é€šè¿‡ LiteLLM Proxy Server è°ƒç”¨")
    print("-" * 50)
    print("âš ï¸  éœ€è¦å…ˆå¯åŠ¨ LiteLLM Proxy Server:")
    print("   litellm --config litellm_config.yaml --port 4000")
    print()
    
    try:
        # è®¾ç½® API base æŒ‡å‘ LiteLLM proxy
        response = completion(
            model="claude-3.7-sonnet",  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­å®šä¹‰çš„æ¨¡å‹å
            messages=[
                {"role": "user", "content": "Hello, who are you?"}
            ],
            api_base="http://localhost:4000",
            api_key="sk-1234",  # å¯¹åº” litellm_config.yaml ä¸­çš„ master_key
            max_tokens=200
        )
        
        print("âœ… è°ƒç”¨æˆåŠŸï¼")
        print(f"å›å¤: {response.choices[0].message.content}")
        print()
        
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤º: ç¡®ä¿ LiteLLM Proxy Server æ­£åœ¨è¿è¡Œ")
        print()


def test_openai_compatible():
    """ä½¿ç”¨ OpenAI SDK è°ƒç”¨ LiteLLM Proxy"""
    print("ğŸ§ª æµ‹è¯• 3: ä½¿ç”¨ OpenAI SDK è°ƒç”¨ LiteLLM Proxy")
    print("-" * 50)
    
    try:
        from openai import OpenAI
        
        client = OpenAI(
            api_key="sk-1234",  # LiteLLM master_key
            base_url="http://localhost:4000"  # LiteLLM proxy URL
        )
        
        response = client.chat.completions.create(
            model="claude-3.7-sonnet",
            messages=[
                {"role": "user", "content": "ç”¨ä¸­æ–‡è¯´ä½ å¥½"}
            ],
            max_tokens=100
        )
        
        print("âœ… è°ƒç”¨æˆåŠŸï¼")
        print(f"å›å¤: {response.choices[0].message.content}")
        print()
        
    except ImportError:
        print("âš ï¸  éœ€è¦å®‰è£… openai: pip install openai")
        print()
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤±è´¥: {e}")
        print()


if __name__ == "__main__":
    print("=" * 50)
    print("LiteLLM + Claude (Bedrock) æµ‹è¯•")
    print("=" * 50)
    print()
    
    # æµ‹è¯• 1: ç›´æ¥è°ƒç”¨
    test_direct_bedrock()
    
    # æµ‹è¯• 2 å’Œ 3: é€šè¿‡ Proxy è°ƒç”¨ï¼ˆéœ€è¦å…ˆå¯åŠ¨ proxy serverï¼‰
    print("ğŸ’¡ å¦‚æœè¦æµ‹è¯• LiteLLM Proxyï¼Œè¯·å…ˆåœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œ:")
    print("   litellm --config api-server-setup/litellm_config.yaml --port 4000")
    print()
    
    user_input = input("æ˜¯å¦æµ‹è¯• LiteLLM Proxy? (y/n): ")
    if user_input.lower() == 'y':
        test_litellm_proxy()
        test_openai_compatible()
    
    print("=" * 50)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 50)
